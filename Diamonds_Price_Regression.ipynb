{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bernardDesmond23/-DiamondsPrice-LinearRegression/blob/main/Diamonds_Price_Regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 1  \n",
        "Download and upload the Diamonds Dataset from Kaggle."
      ],
      "metadata": {
        "id": "v4CeqUae4JQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "df= pd.read_csv(\"diamonds.csv\")"
      ],
      "metadata": {
        "id": "DsN71jhM4KfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2  \n",
        "Clean the data, perform EDA, and retrieve at least 3 insights/observations.  \n",
        "Then, create a random sample of 12,500 records for modelling in a DataFrame named `diamonds_model`.\n"
      ],
      "metadata": {
        "id": "YCaWqEUI4STM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q2: Data cleaning\n",
        "print(\"Dataset Shape\", df.shape)#shows the dimensions of the dataset\n",
        "print(\"\\n First five rows:\")\n",
        "print(df.head(5))\n",
        "print(df.tail(10))\n",
        "print(\"\\n Dataset info:\" )#shows in details the columns datatypes and rows of dataset\n",
        "print(df.info())\n",
        "print(\"\\n Missing values\") #identifies the null values if any\n",
        "print(df.isnull().sum())\n",
        "original_df = df.copy()\n",
        "#check for duplicates\n",
        "print(f\"Duplicate rows: {df.duplicated().sum()}\")\n",
        "print(\"\\n Missing values per column:\")#checks the missing values per column\n",
        "print(df.isna().sum())\n",
        "print(\"\\n statistical summary:\")\n",
        "print(df.describe())\n",
        "#the outliers havent been eliminated because high prices are considered common in diamond price ranges.\n",
        "#because certain daimaonds are of high prices\n",
        "diamonds_model = df.sample(n=12500, random_state=42) #this is a dataframe witha random sample of 12,500 records"
      ],
      "metadata": {
        "id": "xAfnQyj24Zto",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "#INSIGHT 1\n",
        "# Look at price distribution\n",
        "plt.figure(figsize=(10, 4)) # Create a new figure for Insight 1\n",
        "# plt.subplot(1, 2, 1)# 1 row with 2 columns on the position 1 on the left side # Removed subplot as there's only one plot now\n",
        "price_bins = [0, 500, 1000, 1500, 2000, 2500, 3000, 3500, 4000]# price ranges\n",
        "price_labels = ['0-500', '500-1000', '1000-1500', '1500-2000', '2000-2500','3000-3500','3500-4000','4000+']#labels for price ranges\n",
        "df['Income_Category'] = pd.cut(df['price'], bins=price_bins, labels=price_labels)#converts the numerical price into categories\n",
        "\n",
        "price_counts = df['Income_Category'].value_counts().sort_index()#counts the customers in each category\n",
        "#creates the bar graph with the x and y labels with the x axis as the annual income and y axis as the index or number of customers\n",
        "bars = plt.bar(price_counts.index, price_counts.values)\n",
        "plt.title('Distribution of Diamond Prices')#title of the bar graph\n",
        "plt.xlabel('Price')\n",
        "plt.ylabel('Number of Diamonds')\n",
        "for bar, value in zip(bars, price_counts.values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f'{value:.1f}', ha='center', va='bottom')\n",
        "#this plots a boxplot to show outliers like the very expensive diamonds and the cheap diamonds\n",
        "# plt.subplot(1, 2, 2) # Removed subplot for boxplot\n",
        "# sns.boxplot(y=diamonds_model['price']) # Removed boxplot code\n",
        "# plt.title('Price Boxplot') # Removed boxplot title\n",
        "plt.show()\n",
        "#Shows a simple summary of the prices of the diamond\n",
        "print(f\"Average price: ${diamonds_model['price'].mean():.2f}\")\n",
        "print(f\"Most expensive: ${diamonds_model['price'].max():.2f}\")\n",
        "print(f\"Cheapest: ${diamonds_model['price'].min():.2f}\")\n",
        "\n",
        "\n",
        "#INSIGHT 2\n",
        "\n",
        "# Carat vs Price relationship using a scatter graph to show how carat affects price\n",
        "plt.figure(figsize=(8, 5)) # Create a new figure for Insight 2\n",
        "sns.scatterplot(data=diamonds_model, x='carat', y='price', alpha=0.5)\n",
        "plt.title('Bigger Diamonds Cost More?')\n",
        "plt.xlabel('Carat (Size)')#x axis label of the carat\n",
        "plt.ylabel('Price ($)')#y axis label of price\n",
        "plt.show()\n",
        "\n",
        "# Calculate correlation to show how far apart the carat and the price are\n",
        "#in otherwords to show the relationship between carat and price\n",
        "correlation = diamonds_model['carat'].corr(diamonds_model['price'])\n",
        "print(f\"Correlation between carat and price: {correlation:.2f}\")\n",
        "\n",
        "\n",
        "#INSIGHT 3\n",
        "# Cut analysis\n",
        "#\n",
        "plt.figure(figsize=(12, 4)) # Create a new figure for Insight 3\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "# Count how many diamonds of each cut type and makes a bar chart\n",
        "#then it sorts the bars from most common to least common\n",
        "sns.countplot(data=diamonds_model, x='cut', order=diamonds_model['cut'].value_counts().index)\n",
        "diamonds=diamonds_model['cut'].value_counts()\n",
        "bars1 = plt.bar(diamonds.index, diamonds.values,  color=['darkblue'], alpha=0.7)\n",
        "for bar, value in zip(bars1, diamonds.values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f'{value:.1f}', ha='center', va='bottom')\n",
        "plt.title('Most Popular Diamond Cuts ')# title of the bar chart\n",
        "plt.ylabel(\"Number Of Dimonds\")\n",
        "plt.xticks(rotation=45)#tilt the cut names 45 degrees  so they dont overlap\n",
        "\n",
        "#a bar chart that shows which cut types are most expensive on average\n",
        "plt.subplot(1, 2, 2)\n",
        "# Calculate Average prices for each cut type and sorts from highest avaerage price to lowest\n",
        "cut_prices = diamonds_model.groupby('cut')['price'].mean().sort_values(ascending=False)\n",
        "sns.barplot(x=cut_prices.index, y=cut_prices.values)#make bars showing the average prices\n",
        "bars = plt.bar(cut_prices.index, cut_prices.values)\n",
        "for bar, value in zip(bars, cut_prices.values):\n",
        "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
        "             f'{value:.1f}', ha='center', va='bottom')\n",
        "\n",
        "plt.title('Average Price by Cut Quality')#tilte of the bar chart\n",
        "plt.xticks(rotation=45)#tilts the x values to prevent overlap\n",
        "plt.ylabel('Average Price ($)')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Average prices by cut:\")\n",
        "print(cut_prices)"
      ],
      "metadata": {
        "id": "Tb4oWz_fYoYU",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**INSIGHT 1** This is about the diamond price distributions.The price distribution shows most diamonds are concentrated at lower price points . A long tail of increasingly expensive diamonds extends to the right, with very few diamonds above $10,000.This pattern is typical for luxury goods where affordable items dominate the market.\n",
        "\n",
        "**INSIGHT 2** Feature correlation showing carat vs price relationship. There is a strong positive correlation of 0.98 between carat size and price, confirming larger diamonds command higher prices.The relationship appears non-linear, with price increasing exponentially as carat size grows beyond 1.0 carat.This suggests carat weight is one of the most significant predictors of diamond value.\n",
        "\n",
        "**INSIGHT 3** This is about the cut quality analysis.Ideal cut diamonds are the most prevalent in the dataset, indicating market preference for higher quality cuts.Surprisingly, Premium cuts command higher average prices than Ideal cuts, suggesting other factors like rarity or size influence pricing.Cut quality shows a complex relationship with price, not following a simple linear 'better cut = higher price' pattern"
      ],
      "metadata": {
        "id": "eHeg3Fz4cKgb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3  \n",
        "Use Linear Regression to predict diamond prices using all features (carat, cut, color, etc.).  \n",
        "Include feature engineering for categorical variables and verify model accuracy.\n"
      ],
      "metadata": {
        "id": "WGut745P4gEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#  Question 3\n",
        "# import libraries needed for making our linear regression model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import numpy as np\n",
        "\n",
        "#separate carat, cut, and color from price\n",
        "X = diamonds_model.drop(\"price\", axis=1)\n",
        "y = diamonds_model[\"price\"]\n",
        "\n",
        "#Identify the categorical columns(3) and numerical features(6)\n",
        "categorical_features = [\"cut\", \"color\", \"clarity\"]\n",
        "numerical_features = [\"carat\", \"depth\", \"table\", \"x\", \"y\", \"z\"]\n",
        "\n",
        "#preprocessing pipeline to transform the categorical data column\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"cat\", OneHotEncoder(drop=\"first\"), categorical_features)\n",
        "    ], remainder=\"passthrough\"\n",
        ")\n",
        "\n",
        "#connecting the preprocessing step to our regression model\n",
        "model = Pipeline(\n",
        "    steps=[(\"preprocessor\", preprocessor), # encodes cut, color and clarity.\n",
        "     (\"regressor\", LinearRegression())] # applies linear regression to the processed data\n",
        ")\n",
        "\n",
        "#spliting the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "#training the model\n",
        "model.fit(X_train, y_train)# .fit() automatically applies preprocessing first then trains the model.\n",
        "\n",
        "#making predictions on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "#Evaluation of the mean absolute error, mean squared error, root mean squared error and R² score\n",
        "mae = mean_absolute_error(y_test, y_pred)\n",
        "mse = mean_squared_error(y_test, y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "print(\"Model Performance Metrics:\")\n",
        "print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "print(f\"R² Score: {r2:.4f}\")\n",
        "#compare the first 10 actual samples to the first 10 predicted samples\n",
        "comparison = pd.DataFrame({\"Actual Price\": y_test[:10], \"Predicted Price\": y_pred[:10]})\n",
        "print(\"\\nSample Comparison of Actual vs. Predicted Prices:\")\n",
        "print(comparison)"
      ],
      "metadata": {
        "id": "u_4e6lfv4led",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "R² score = 0.9237\n",
        "Our linear regression model explains 92.37 percent of the variation in diamond prices based on the carats, cut, color, clarity and dimensions which is regarded high for pricing data.\n",
        "Mean Absolute Error(MAE) = 724.70\n",
        "This is the average difference between the actual price and our model's predictions.\n",
        "Root Mean Squared Error(RMSE) = 1100.41\n",
        "Simiar to MAE but squares MAE before averaging it.\n",
        "Our RMSE is less than MSE suggests they are few outliers, in this case very cheap or very expensive diamonds."
      ],
      "metadata": {
        "id": "IohpVDsrZIup"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 4  \n",
        "Apply PCA to select the 2 most suitable continuous features correlated with price.  \n",
        "Model this relationship using Linear Regression and verify accuracy.\n"
      ],
      "metadata": {
        "id": "DBewrCue4reZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q4: PCA  with selected features\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "num_features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
        "X_num = diamonds_model[num_features]\n",
        "y = diamonds_model['price']\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_num)\n",
        "\n",
        "#PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "print(\"Explained variance ratio:\", pca.explained_variance_ratio_)"
      ],
      "metadata": {
        "id": "Rnr7xWg04yon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model this relationship using the linear regression algorithm and verify its accuracy.\n",
        "#imports already in q3\n",
        "#Split the Data\n",
        "X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "\n",
        "#Train the Model\n",
        "pca_model = LinearRegression()\n",
        "pca_model.fit(X_train_pca, y_train)\n",
        "y_pred_pca = pca_model.predict(X_test_pca)\n",
        "\n",
        "#Verify Accuracy\n",
        "rmse_pca = np.sqrt(mean_squared_error(y_test, y_pred_pca))\n",
        "\n",
        "print(\"PCA-based model R²:\", r2_score(y_test, y_pred_pca))\n",
        "print(\"PCA-based model RMSE:\", rmse_pca)"
      ],
      "metadata": {
        "id": "Ev0cx7DmgXLB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 5  \n",
        "To the model developedd in the question 3, fit 2 different regression models using Lasso and Ridge regularisation techniques and validate accuracies for both models.\n"
      ],
      "metadata": {
        "id": "vjJ2JGUT45Cr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Q5: Fitting Ridge and Lasso regression models to model developed in Question 3\n",
        "#import ridge and Lasso\n",
        "from sklearn.linear_model import Ridge, Lasso\n",
        "#Ridge Regression Model\n",
        "ridge_model = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"regressor\", Ridge(alpha=1.0, random_state=42))\n",
        "    ]\n",
        ")\n",
        "ridge_model.fit(X_train, y_train)\n",
        "ridge_pred = ridge_model.predict(X_test)\n",
        "\n",
        "#Lasso Regression Model\n",
        "lasso_model = Pipeline(\n",
        "    steps=[\n",
        "        (\"preprocessor\", preprocessor),\n",
        "        (\"regressor\", Lasso(alpha=0.001, random_state=42, max_iter=10000))\n",
        "    ]\n",
        ")\n",
        "lasso_model.fit(X_train, y_train)\n",
        "lasso_pred = lasso_model.predict(X_test)\n",
        "\n",
        "#Evaluate Models\n",
        "def evaluate_model(y_true, y_pred, name):\n",
        "    mae = mean_absolute_error(y_true, y_pred)\n",
        "    mse = mean_squared_error(y_true, y_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    print(f\"\\n{name} Model Performance:\")\n",
        "    print(f\"Mean Absolute Error (MAE): {mae:.2f}\")\n",
        "    print(f\"Root Mean Squared Error (RMSE): {rmse:.2f}\")\n",
        "    print(f\"R² Score: {r2:.4f}\")\n",
        "    return r2\n",
        "\n",
        "ridge_r2 = evaluate_model(y_test, ridge_pred, \"Ridge Regression\")\n",
        "lasso_r2 = evaluate_model(y_test, lasso_pred, \"Lasso Regression\")"
      ],
      "metadata": {
        "id": "ObH8sL6y46HH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both Ridge and Laso models are performing excellently. The difference is in the small rounding errors.\n",
        "R² score of 0.923 means that 92.3% of the variance in diamond prices is explained by carats, cut, color, clarity and depth.\n",
        "According to the outputs:\n",
        "- For MAE Ridge is slighty better than Lasso since it stands at 724.38 as compared to 724.69 for Lasso.\n",
        "- For RMSE Lasso is slightly better than ridge since it is 1100.41 as compared to 1102.00 for Ridge.\n",
        "- For R² score Lasso is slightly better since it is 0.9237 compared to 0.9234 for Ridge."
      ],
      "metadata": {
        "id": "XGRcIjzB78-T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 6  \n",
        "Compare the 4 models (Linear Regression, PCA Regression, Lasso, Ridge).  \n",
        "Identify which achieved the highest accuracy and explain why.\n"
      ],
      "metadata": {
        "id": "sH6_irIk5A6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "if 'diamonds_model' not in locals():\n",
        "    diamonds_model = pd.read_csv(\"diamonds.csv\")\n",
        "\n",
        "\n",
        "num_features = ['carat', 'depth', 'table', 'x', 'y', 'z']\n",
        "X_num = diamonds_model[num_features]\n",
        "y = diamonds_model['price']\n",
        "\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_num, y, test_size=0.2, random_state=42)\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X_num)\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "X_train_pca, X_test_pca, y_train_pca, y_test_pca = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
        "pca_model = LinearRegression()\n",
        "pca_model.fit(X_train_pca, y_train_pca)\n",
        "y_pred_pca = pca_model.predict(X_test_pca)\n",
        "rmse_pca = np.sqrt(mean_squared_error(y_test_pca, y_pred_pca))\n",
        "\n",
        "\n",
        "ridge_r2 = 0.9234\n",
        "lasso_r2 = 0.9237\n",
        "\n",
        "\n",
        "comparison_results = pd.DataFrame({\n",
        "    'Model': ['Linear Regression', 'PCA Regression', 'Ridge Regression', 'Lasso Regression'],\n",
        "    'R² Score': [\n",
        "        0.9237,\n",
        "        r2_score(y_test_pca, y_pred_pca),\n",
        "        ridge_r2,\n",
        "        lasso_r2\n",
        "    ],\n",
        "    'RMSE': [\n",
        "        1100.41,\n",
        "        rmse_pca,\n",
        "        1102.00,\n",
        "        1100.41\n",
        "    ]\n",
        "})\n",
        "\n",
        "print(\"Model Comparison Summary:\\n\")\n",
        "print(comparison_results.sort_values(by='R² Score', ascending=False))\n",
        "\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Model', y='R² Score', hue='Model', data=comparison_results,\n",
        "            dodge=False, palette='coolwarm', legend=False)\n",
        "plt.title('Model Accuracy Comparison (Higher R² = Better)')\n",
        "plt.ylabel('R² Score')\n",
        "plt.xlabel('Regression Model')\n",
        "plt.xticks(rotation=20)\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Model', y='RMSE', hue='Model', data=comparison_results,\n",
        "            dodge=False, palette='crest', legend=False)\n",
        "plt.title('Model RMSE Comparison (Lower RMSE = Better)')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Regression Model')\n",
        "plt.xticks(rotation=20)\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "vFS5o2Aw5CBC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation:**\n",
        "The comparison shows that both Linear and Lasso Regression models achieved the best performance with R² ≈ 0.9237 and RMSE ≈ 1100. Lasso slightly outperforms others because its L1 regularization reduces overfitting while keeping key predictors. Ridge performed marginally lower (R² ≈ 0.9234) due to stronger coefficient shrinkage, while PCA Regression had lower accuracy (R² ≈ 0.85) because it lost some information after dimensionality reduction. Overall, Lasso Regression provides the most balanced and accurate prediction model.\n"
      ],
      "metadata": {
        "id": "_mAorfWkuPwp"
      }
    }
  ]
}